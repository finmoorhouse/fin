---
title: EA Projects I'd Like to See
permalink: /writing/ea-projects/
tags: [writing]
date: 2022-03-13
featuredImage: './ea-projects-share.png'
---
import Sidenote from "components/writing/sidenote";

import { Link } from "gatsby"



I think sharing half-baked ideas is an underrated practice (see my post on <Link to='../research'>research ideas</Link>)<Sidenote label='harm'>Typical caveats apply: make you're not doing accidental harm by spreading bad ideas.</Sidenote>. So I've written out some ideas for concrete projects that have been floating around my head for a while.<Sidenote label='FTX'>[Effective altruism](https://www.effectivealtruism.org/), especially [longtermist](https://longtermism.com/) EA, needs ideas for new, *scalable*, *[ambitious](https://80000hours.org/articles/be-more-ambitious/)* projects — because of [announcements like this.](https://forum.effectivealtruism.org/posts/2mx6xrDrwiEKzfgks/announcing-the-future-fund-1) Unfortunately many of these ideas are neither especially scalable nor especially ambitious.</Sidenote>.

If you could see yourself actually working on one of these, please do let me know! I might be able to connect you to other people who could work with, elaborate on details, or help you find funding.

I expect this will end up being part 1 of $$N$$, where $$\mathbb{E}(N)\approx 3$$ this year. Note that the order is mostly arbitrary.

## Longtermist visualisations

Imagine a ‘timeline of everything’, showing major events (astronomical, geological, historical) from the Big Bang to the end of time. Users can zoom in and out, much like existing apps that show the [scale of the universe](https://shop-us.kurzgesagt.org/products/universe-in-a-nutshell-app).

Appreciating the vast amount of time ahead of us, and the relatively brief period of time that all of recorded human history makes up, is a key underlying intuition for longtermist arguments. The website could explain longtermist ideas as link to relevant reading, like *[The Precipice](https://theprecipice.com/)*.

Various timelines of the universe have been made in video or graphic form, such as [here](https://www.youtube.com/watch?v=uD4izuDMUQA). But I suspect being able to navigate through different scales of time yourself might be a very different experience.

Ultimately, you could imagine a website hosting a series of visualizations illustrating various longtermist ideas. These visualisations and graphics, some interactive, some updated with live data, could be tied together with essays about key longtermist topics, amounting to a kind of undirected, highly visual introduction to ideas from effective altruism and longtermism. You could imagine a handful of ‘tracks’ (e.g. big history, existential risks, technological progress, human progress) which tie together these graphics, once enough are made. I am excited by the prospect of [Our World In Data](https://ourworldindata.org/) adding new charts which could be of special interest from a longtermist perspective. But I do think this project could be different enough to warrant being separate, because the visualisations could be more creative, various in form, and perhaps less squarely data-driven.

I made a small start on this idea last year, and made plans to hire for it, but left it by the wayside because I got distracted.

## Meta book ideas

### Book grantmaking

As I understand it, a lot of involved work is required to secure a book deal, and ∴ to start on the process of writing the book. Before you can get an advance from one of them, you need to shop an idea around publishers, or find an outside agent you can trust to do so. The author typically needs to put a lot of their own time into this process, and in any case will need to wait before a deal goes through to begin writing with the confidence that their time is being put to good use. First-time or more obscure authors have it especially bad, since they have little to show to prospective publishers. This is presumably bad, at best because it uses up the time of people who's time is valuable and could be used just writing the book; and at worst because it makes potentially impactful books less likely to happen in the first place, for this reason.

In this new context for EA where money appears to be much less of a constraining factor, I wonder if this problem can be fixed. Imagine a group of evaluators with (i) a good amount of context on EA ideas, and (ii) a decent understanding of the world of publishing. As a prospective EA author, you apply with your book idea to this team, and if the pitch meets a basic threshold, then you quickly receive an advance. After that, the work of finding a publisher falls to this team of specialists, rather than the author herself. But the author retains the rights to the book if/when it is eventually published. If the group cannot find a publisher after some period of time, they have the option to self-publish, e.g. as a free ebook.

The major effect of such a scheme is that it would very likely **make more valuable books happen**.

Primarily, this is because the bar for which books get approved would be lower: more valuable book ideas would be quickly approved by this group than by the average publisher. The main reason the bar would be lower is that the group would not prioritise profits; up to and including the point where it could make sense to commission books which will not take a profit in expectation (but would nonetheless benefit the world as e.g. a free ebook). Another reason is that some ideas related to effective altruism might be hard to properly convey to a publisher without much context — the book could be saying something very important, and important enough that the idea could catch on and sell well, but there's a greater chance that a conventional publisher misses this. *[Superintelligence](https://www.goodreads.com/book/show/20527133-superintelligence)* might count as an example — it ended up catching on in a way that was very hard to predict, a grantmaker with a lot of context on the ideas might have anticipated that better than a mainstream publisher.

The other reason this scheme might help more valuable books happen is because it just eliminates much of the administrative faff of each prospective author figuring out on their own how to get started.

When it comes to books and other media like films and social media accounts, I think a hits-based approach is best. I would guess that (i) the impact of books is roughly power-law distributed<Sidenote>Certainly book sales are heavy-tailed, and sales are going to track impact decently well.</Sidenote>, and (ii) the expected impact of a book doesn't scale linearly with the amount of money you put into it. These two things would suggest that it would make sense to roll the dice on many book ideas which could plausibly do well.

Centrally, then, the idea is to install a middleman between EA authors and publishers, capable of smoothing out the risks for the authors by handling multiple books at once and being less sensitive than individual authors to losing money.

What's the case against? Maybe I've overrated how much hassle it is to find a publisher in order to get started on a book. Maybe I have also overrated how many potentially very valuable EA books there are waiting to happen, but which don't happen for the reasons discussed. It might also just be the case that nearly all the most promising potential authors would do better to spend their time another way, and that there aren't so many other promising potential authors.

I consider a couple similar ideas below.

### Buying back rights

10 years after the publication of Peter Singer's *[The Life You Can Save](https://www.thelifeyoucansave.org/the-book/)*, the organisation of the same name bought back the copyrights to the book. As a result, they could distribute the book for free, and record and release a free (and star-studded) audiobook.

As I understand it, some of the legal/administrative aspects of the deal posed a major and time-consuming difficulty. But the difficulty per book will decrease the more books we do this for (assuming there are ways to learn from and systematise the process). So perhaps we should try doing this for more books for which it would be really valuable to hand out free copies (or hand out physical copies with much less hassle). 

### Translations

*The Precipice* doesn't have a Spanish translation. It should.

As I understand it, the way books typically get released in new languages is that a publisher in a new language will make a bid on the book and supply the translator(s) themselves. If the bid makes sense, the book's agent (sometimes in consultation with the author) will sell the copyrights for that region, and the book gets republished.

Translating important books just seems extremely worthwhile. Try thinking of your list of the ten most important books of the last couple decades — the books you wish everyone would read. Imagine one of those books lacks a translation in a language with > 50 million first-language speakers. What's the expected impact, as a fraction of the impact of the original book? Is it greater than 1%? That seems plausible in at least a handful of cases. Now how much should an altruistic planner have been prepared to pay to make the original book exist? What is 1% of that cost? Likely still very high.<Sidenote label='book-price'>My own feeling is that e.g. *Superintelligence* and *The Precipice* are each worth > $1 billion in this sense. $10 million is indeed still very high.</Sidenote>

So the default course of waiting for offers to come around seems improvable — partly because we could help make translations happen which might never happen otherwise, but also to accelerate their arrival where they would have happened anyway. But that's not all: the impact of a translated book can depend on the accuracy of its translation. Terms of art and arguments are typically crafted with some care, and fragile to small errors in translations. So in taking matters into our own hands, we might also select translators trusted to get the finer and more idiosyncratic details right.

In practice, I'm imagining something like proactively approaching major publishers in (typically) non-English speaking countries and introducing them to a translator who we trust will do an excellent job, and who we can commit to paying ourselves. But I'm sure there are plenty of variations.

### An EA publishing house

The question that spawned these book-related thoughts was: **what if EA started a publishing house?**

I'm not *sure* this is a good proximate aim, because It's unclear why you'd want to replace many of the functions that established publishers already serve. For instance, many publishers are well-known, and getting your book published by them counts as free credibility and publicity. And as mentioned, publishers operate through relationships which take a long time to establish.

But it would be *very cool* to have close to an end-to-end publishing operation. To begin with, this would get rid of some of the friction and frustration associated with grappling with an outside editor over content, cover imagery, and so on. And it would be very easy to reprint classics<Sidenote label='classics'>Like Mill's [*On Liberty*](https://www.goodreads.com/book/show/385228.On_Liberty) or the collected works of Alan Turing, or whatever else.</Sidenote> in the public domain, in order to bring them to a wider audience. Collected blog posts, also<Sidenote label='blogs'>A beautiful hardback selection of [Gwern](https://www.gwern.net/) essays, anyone? Or more [SSC](https://slatestarcodex.com/)/[ACX](https://astralcodexten.substack.com/) essays in book form?</Sidenote>.

Plus, you could spin up your own badass brand, and shape it however you like. You could quickly earn [1,000 true fans](https://kk.org/thetechnium/1000-true-fans/) who automatically buy the next book you release. I think that the real impact you get from books is heavy-tailed in the sense that almost all the impact comes from a small number of readers. If you can cultivate the brand to appeal to those few readers, you don't need a very large audience to get the 'impact-adjusted reach' of a well-established publishing house.

Some people did successfully publish [some highlights](https://www.lesswrong.com/books/2018) from LessWrong, and they [just did it again](https://www.lesswrong.com/posts/mvPfao35Moah8py46/book-launch-the-engines-of-cognition). But I imagine this would have been easier, and potentially reached more people, if a specialised initiative had existed with some of the infrastructure and expertise already in place.

The obvious and standout inspiration here is the (hopefully not) inimitable [Stripe Press](https://press.stripe.com/)<Sidenote label='stripe-press'>Incidentally I think their website is the prettiest arrangement of pixels I've ever seen.</Sidenote>.

A spin on this idea could be to start an '[imprint](https://en.wikipedia.org/wiki/Imprint_(trade_name))' instead — a new 'trade name' for an existing publisher. For instance, [Viking Press](https://en.wikipedia.org/wiki/Viking_Press) is an imprint of [Penguin Random House](https://en.wikipedia.org/wiki/Penguin_Random_House).[^1]

[^1]: There might also be ways to help **market and promote** EA books better. As I understand it, publishers rarely put a lot of effort into marketing a median book. More precisely, they shovel nearly all their marketing efforts into the few books they expect to perform best. So if you want to market your book properly, you really need to use a marketing agency: a firm that specialises in marketing your kind of book. But this still isn't entirely straightforward, for the familiar reason that the marketing agency might need to absorb an unusual amount of context in order to get the message dead-on. Therefore, it might be useful to have some 'in-house' marketing or promotion outfit, that interacts with established marketing agencies to generate and push on EA-specific promotion ideas. In the short-term, this should probably be a complement rather than a replacement for outside marketing agencies, because much or nearly all of the value a marketing agency offers comes from their extensive relationships with e.g. media outlets, which take a ton of time to build. To pull off some kind of in-house marketing agency, it would probably be necessary to hire industry insiders, who could bring their contacts, expertise, and credibility. Another way to help could be to provide **user testing** with books — sending passages or the entire book to EA volunteers for feedback. Normally it's difficult to solicit a lot of high-quality feedback while you are writing a book, but EA has the very special fortune to be made up of folks who are capable of giving useful feedback even on somewhat involved or technical pieces of writing. This is all in a footnote because I'm much less excited about it.

In the meantime, I expect it would also be obviously good for prospective EA authors to pitch ambitious book ideas to existing badass publishing houses like Stripe Press.

## Buying articles

If you wanted to spread ideas you thought mattered, and you happened to have [$250 million](https://www.forbes.com/sites/stephaniedenning/2018/09/19/why-jeff-bezos-bought-the-washington-post/?sh=6b0ba0c93aab) burning a hole in your pocket, you might consider buying a media company like a newspaper, such as *The Atlantic*. I expect there are far most cost-effective ways of spreading good ideas, because this strategy is so untargeted. When you buy a newspaper, you are buying all the showbiz and health and beauty columns which you can't do anything especially useful with. Your plan might be to (i) to begin taking an editorial line on certain verticals of the newspaper in the direction of the ideas you cared about, and (ii) to promote and open-access the articles that come out of (i). But buying the whole company to do this is like buying a cruise ship to execute a beach landing.

But maybe there’s a more targeted version that could work.

Some newspapers featured [sponsored articles,](https://www.theguardian.com/info/2016/jan/25/content-funding#3758127a-47fa-4242-9daf-087de11eb552) a kind of ‘native advertising’ where a company pays to either commission or write an article, which is released open-access on the newspaper’s website, along with a sticker indicating that “this post was sponsored by company X”. By 'open-access', I mean removing the paywall for that specific article.

What about a [philanthropic](https://www.theguardian.com/info/2018/oct/02/philanthropic-partnerships-at-the-guardian) version of this, where someone sponsors stories about EA topics to be published and open-accessed on popular newspapers like The Guardian or The Atlantic. Philanthropic organisations do support content on major media outlets, but I'm not aware of them paying for articles to be open-accessed. This strategy might be used to quickly spread good ideas about effective altruism.

An initial version could involve open-accessing existing articles. But you could also imagine creating an ongoing relationship with a media company where it is increasingly possible to commission articles, because the company makes more from open-accessing the ‘good’ articles than the revenue from ads they would otherwise have made.

I don’t think many articles have been ‘sponsored’ like this for philanthropic reasons, but I can’t think of why it wouldn’t be possible to do more (if not actually a good idea).

## Overlay Journal

An [overlay journal](https://en.wikipedia.org/wiki/Overlay_journal) is a journal (almost always exclusively online) that does not produce its own content, but selects from texts that are already (freely) available online. The selection process can look just like that of a ‘real’ journal, including a board of editors and peer review.

Fields within existential risk, AI safety, global priorities research, and other aspects of EA<Sidenote label='progress-journal'>Progress studies also.</Sidenote> lack dedicated journals. Yet, it’s often difficult to get such work published in decent journals because of the interdisciplinary nature of the work, because the work is unusually speculative, or because of its perceived weirdness.

That’s an issue, because research exerts influence through prestige and citations, and prestige and citations are more likely if you are published in a top journal, in part because top journals are read more.

This tentatively suggests the idea of establishing new academic journals for EA fields. But I'm not sure about that. Prestige is hard to quickly bootstrap with money, setting up a functional journal actually just sounds like a lot of hard administrative work, and any such journal would need an editorial board of prominent researchers in the field, whose time would likely be better spent doing anything other than editing a niche journal.

But maybe there is a neighbouring idea which could work: an online overlay journal, which could more quickly and easily become widely read and earn prestige or acclaim. This is because overlay journals do not compete with proper journals, but rather select from them. You could imagine the website being really attractive, and each issue could be filled with commentary from the authors and editors. Illustrations even. The product is a collection of articles designed to actually interest people in or adjacent to the field; but it is not entering the game of competing with established journals for status. 

Most overlay journals select from open-access research<Sidenote label='overlay-examples'>Such as the [*Journal of High Energy Physics*](https://en.wikipedia.org/wiki/Journal_of_High_Energy_Physics), [*Logical Methods in Computer Science*](https://en.wikipedia.org/wiki/Logical_Methods_in_Computer_Science), and [*Geometry & Topology*](https://en.wikipedia.org/wiki/Geometry_%26_Topology), which are all overlays for [arXiv](https://arxiv.org/).</Sidenote>. But I think it could also be possible to share articles from paywalled journals, by paying an '[article processing charge](https://en.wikipedia.org/wiki/Article_processing_charge)'. My (outsider) impression is that it's somewhat rare to pay to 'liberate' your article from a paywalled journal, because you can often access it for free if you belong to an academic institution which is paying a subscription to the journal, or otherwise you can use evil and nefarious means like [Sci-Hub](https://sci-hub.hkvisa.net/). But paying to free high-quality research from their paywall silos could make sense if it meant that the research became just a bit more well-known, or easier to cite. If you think the research is really important, and you have the money to do so, then this could be a good buy<Sidenote label='overlay-sidenote'>You could definitely do this without making it part of the broader overlay journal idea.</Sidenote>.

Done well, this could help important research get eyeballs of researchers in adjacent fields, and project a certain amount of credibility. It could also provide a strong ‘worst case’ or fallback home for really good research which doesn’t have a natural home. To the extent that research is sometimes not done for this reason, or less effort is put into it, an overlay journal could incentivise quality EA research.

Take Toby Ord's recent paper ‘[*The Edges of Our Universe*](https://arxiv.org/abs/2104.01191)’. It’s valuable, fun work; and Toby was able to write it in part because he was not worrying about climbing the academic ladder with publications. It currently lives on arXiv, an open-access repository which anyone can post papers to without review. If someone more junior than Toby was considering whether to write this, it could be more likely that they decided not to.

A related idea is an open-access online magazine which *rewrites* technical work in e.g. philosophy, economics, bio, into accessible and engaging articles. [*Distil*](https://distill.pub) exemplifies how to do this for some AI research. I think I would be at least as excited about this idea as the overlay journal idea. 

A related idea is to flat-out buy a big journal, or one of the [publishing companies](https://en.wikipedia.org/wiki/Elsevier) that owns them. Maybe I'll write about that in the next iteration!

## Funding criticism of effective altruism

**Note: this is now a thing! I helped set up a [contest for criticism of effective altruism](https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming), with at least $100k in prizes. The deadline is September 1st — consider applying!**

I think one of the most valuable features of EA is its epistemic culture: the way EAs reason about hard problems. I feel unusually free to discuss close to anything that seems important; I worry an unusually small amount about offending my superiors or saying something that could easy be taken out of context, accidentally offending my peers by disagreeing with them, or about saying naive things with the aim of being corrected. I do not believe this culture is guaranteed to persist without effort to maintain it, which I think should include continuing to foster a culture of openly questioning and criticising crucial assumptions. In particular, I don't want anyone within or beyond EA to have good reason to worry about the consequences of voicing good-faith criticisms. Positively encouraging and funding those criticisms, and similar 'red-teaming'<Sidenote label='red-teaming'>Note that ['red teaming'](https://en.wikipedia.org/wiki/Red_team) is standard (and important) practice outside of the nonprofit context, specially in cybersecurity & intelligence.</Sidenote> activities is a clear way to address that.

Even without any reason to worry about saying critical things, useful criticisms are still probably going to be underprovisioned. This is for obvious reasons: where are the naturally occurring incentives to point out where something is going wrong, unless we deliberately set them up?

Plus, clearly, critical work is often just highly valuable, because:

- The set of ideas that makes up effective altruism and longtermism is still relatively new, and still being developed. It would not be surprising if the ‘consensus’ has not missed certain crucial considerations, or if some parts of that consensus were mistaken.
- Sometimes it’s possible to make mistakes in implementing a project which aren’t obvious from the inside, but can be pointed out by outside observers. This could be especially true for longtermist projects, which may have fewer obvious feedback loops.
- Points of confusion go under-reported. It can be embarrassing to announce that you're simply confused about some assumption that everyone else seems to regard as obvious. I expect there's some amount of [pluralistic ignorance](https://en.wikipedia.org/wiki/Pluralistic_ignorance) at play here, and the less of this the better.
- The epistemic state of longtermist EA could be much improved. Longtermism strikes me as an area where it may be easy to perceive more consensus and clarity than really exists. So we should be excited about suggestions about what the longtermist research community has missed so far.
- Because (especially longtermist) EA has grown so rapidly, big projects now might be grounding their theories of change in a relatively small amount of early research. It makes sense to check that research for [Wizard of Oz](https://www.youtube.com/watch?v=NZR64EF3OpA) style citation trails — where the key claims bottom out in citations that aren't much better than guesswork. 
- A good deal of research which informs EA decision-making is conducted by generalists rather than subject experts. We might benefit from employing subject experts to review that research and point out blind spots.

I think the best way to collect low-hanging fruit here is with a prize, probably announced and conducted on the [EA Forum](https://www.forum.effectivealtruism.org). The idea would be to solicit pieces critically assessing some existing work, ideas, directions, or empirical claims. 1,000–10,000 words, say; 2–20 hours of work. A prize pool of (say) $20,000 to $150,000, to be disbursed between multiple winners.

The prize announcement should be clear about the criteria; and the criteria might include: [reasoning transparency](https://www.openphilanthropy.org/reasoning-transparency)<Sidenote label='transparency'>How much expertise do you have? How confident are you about the claims you’re making? What would change your mind? If your work includes data, how were they collected?</Sidenote>, novelty, action-relevance<Sidenote label='action-relevance'>It’s fine to point out where something is going wrong; even better to be constructive, by suggesting a concrete improvement.</Sidenote>, and focus (well-defined scope). I think the [EA Forum guidelines](https://forum.effectivealtruism.org/posts/Y2iqhjAHbXNkwcS8F/how-to-use-the-forum) are an excellent and relevant resource also.

But you could also imagine offering **grants** for individuals to write critical pieces. This might involve disagreeing with an influential view or piece ("I dispute X"), or it could involve engaging with a neglected perspective or discipline ("X seems to have missed Y, here's how you can add it in"). There could be a fund people could apply to with research proposals, and/or that fund can proactively reach out to individuals who might be a good fit. There could also be some means by which anyone can recommend some other person for consideration.

But useful criticisms don't only go unwritten because it's hard to get paid for them: just paying people doesn't address concerns about prestige and status. To address those worries, EA (research) organisations might be encouraged to create a /critique-us page on their website, where they list key claims that they would most like to see scrutinised, and positively encourage people to critically investigate them.

Also, it should be easy to write pseudonymously or anonymously. The [Journal of Controversial Ideas](https://journalofcontroversialideas.org/) allows this, and is commendable in that respect.

Who decides what criticism gets funded, published, and awarded? That's tricky. Some selection *would* be needed, because we specifically want to reward (and so incentivise) thoughtful, useful, good-faith criticism. Just boosting critical voices across the board would drown the useful signal in attention-draining noise. One model is to have a board composed of insiders and outsiders to EA, who vote on applications for funding or winners of a prize. Whatever the model ends up being, it seems unusually important for the decision-making process to be transparent; meaning who was not recommended for funding should be publicly knowable (if the rejected applicant so wishes).

Lastly, what should happen to this critical work once it's written? How do we make sure decision-relevant criticism change decisions? One answer is to try collating the work in a single place, such that criticisms are easy to search and find. I also think it would be useful to have some means by which it's easy to identify **responses** to, **changes** caused by, and **discussions** of any piece of criticism, as far as there are any. This could just live on the forum — there are already [some relevant tags](https://forum.effectivealtruism.org): for [criticism of effective altruist organizations](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-organizations), [criticism of effective altruist causes](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruist-causes), and [criticism of effective altruism](https://forum.effectivealtruism.org/tag/criticism-of-effective-altruism). To post criticism there would be a good start. But I expect we could do more. For instance, the project could involve a dedicated website, which lists each piece with some meta-commentary, links to discussion on the EA Forum, related pieces, and so on. It could even become a journal, although I suspect this might be jumping the gun given the absence of a journal for any part of EA itself.

## Space governance research centre

I am entirely biased, because I recently wrapped up a [preliminary bit of research](https://80000hours.org/problem-profiles/space-governance/#research-organisations-interested-in-space-governancefn-24) on space governance. But I think the governance of outer space could end up becoming something like a new research area for longtermist EAs.<Sidenote label='not-as-big'>Though not close to the scale of AI safety, AI governance, or bio.</Sidenote>

When an extremely nascent field is just coalescing, I think it makes sense to find everyone who could help establish it, and put them in the same place (figuratively or literally). Minimally, this could look like reaching out to people who seem to know a bit about space and a bit about the principles of longtermist EA, and hooking them into an informal network of researchers.

A more ambitious version could look like a proper research centre — i.e. actually registering a nonprofit, having people on the payroll, having a bunch of branding and operational support and maybe even affiliations with established academic institutions.<Sidenote label='fhi'>[FHI](https://www.fhi.ox.ac.uk/) in the Wild West days.</Sidenote>

One reason is that this could bootstrap the field very quickly, and indeed increase the chance a field emerges at all.<Sidenote label='field'>Where, to be clear, the field is something like 'longtermist considerations on space governance'.</Sidenote>

## Book ideas

In general, I think specific book ideas aren't wildly valuable, because almost all the work, and all the value, comes from the execution rather than the idea. Regardless, here are some specific EA-relevant books I would love to read.

### A (New) Introduction to Effective Altruism

*[Doing Good Better](https://www.effectivealtruism.org/doing-good-better)* is a superb book, and probably remains the best book-length introduction to effective altruism. But in part because the book did so much to grow EA, the movement has outgrown the book. This is neither a surprise nor a mark against the book — nearly 7 years have passed since it was published.

In particular, the book came before AI alignment, existential risks, and [longtermism](https://longtermism.com/) really emerged as key threads in EA thinking, so the experience of reading the book and then getting up to speed with cutting-edge EA could be jarring; even confusing<Sidenote label='wwotf'>*The Precipice* and *What We Owe The Future* (forthcoming) do focus on the longtermist aspect, but they are not centrally about effective altruism the project/community/bundle of ideas.</Sidenote>. The issue isn't simply that longtermism happened — the community itself has properly grown up since 2015, along with a constellation of organisations, ideas, cause areas, and cause area candidates. If your aim is to excite people about getting involved with EA, failing to describe all the exciting stuff which has happened in the past 5 years would be to miss a wide-open goal.

What should a new introduction to EA look like? Well, I'm confident that it shouldn't look like a redo of *Doing Good Better*<Sidenote label='dgb'>Doing *Doing Good Better* better.</Sidenote>. One cool version could be a whirlwind survey of organisations and concepts, where the aim is just to blow the reader's mind as efficiently as possible. Or it could in fact be multiple books, where the arguments and framings in each are more specifically tested and optimised for specific audiences; such as high school kids looking for inspiration about what to do with their lives.

I also happen to think this is unusually important to get right, since the reputation of the book would be difficult to separate from EA's overall reputation. So I think a lot of care should be taken in writing it, and likely the idea should be entrusted to someone with a track record as a communicator.

### Utilitarianism: a Modern Introduction and Defence

I'm still a bit confused about why there are not more popular books about utilitarianism. It does seem clear that utilitarian views are decidedly out of vogue in the philosophical establishment, and in the humanities writ large. About more obscure ethical views you will find enough books, trade and academic, to fill flea market bookshelves. But the best philosophical explanation and defence of utilitarianism may literally be Mill's (1861) [*Utilitarianism*](https://www.goodreads.com/en/book/show/584637)<Sidenote label='utilitarian-books'>Here's a [list of other options](https://www.goodreads.com/shelf/show/utilitarianism). Also worthy of mention are J.J.C. Smart's *[Utilitarianism: For and Against](https://www.goodreads.com/book/show/365649.Utilitarianism)*, Peter Singer's writing (especially [*Practical Ethics*](https://www.goodreads.com/book/show/29378.Practical_Ethics)), [Katarzyna De Lazari-Radek](https://www.goodreads.com/author/show/8271243.Katarzyna_De_Lazari_Radek)'s *[Very Short Introduction](https://www.goodreads.com/book/show/34749756-utilitarianism)* and the writing of [Joshua Greene](https://www.joshua-greene.net/), especially [*Moral Tribes*](https://www.goodreads.com/book/show/17707599-moral-tribes).</Sidenote>.

I claim this is bad, and not in a way which depends on a full-blooded utilitarianism being correct. I think of 'utilitarianism' as now mostly referring to a somewhat diffuse bundle of attitudes; centrally **scope sensitivity**, **impartiality**, some kind of **aggregative principle**, a focus on **consequences**, some kind of **total view** with respect to population ethics, and some kind of **[Bayesian mindset](https://www.cold-takes.com/the-bayesian-mindset/)**. Taken independently, it just seems obviously good if more people had a better understanding of the virtues of each of these components.

I'm less interested in defending utilitarianism against edge cases and restrictions, because I think the important core of the view is rarely undermined if you allow those edge cases and restrictions. If most people thought the Earth was flat, and you claimed it was round, you wouldn't mind conceding that it wasn't exactly spherical.

Outside the academy, 'utilitarianism' and 'utilitarian' connote bad things, and I think it's time to reclaim those words. For instance, 'utilitarian' suggests 'cold and austere', but utilitarian-as-in-ethically-utilitarian design [would surely be really fun](https://twitter.com/AmandaAskell/status/1486790363834556422). More seriously, utilitarianism is associated with a calculating approach, as opposed to a loving one. But there's a sense in which utilitarianism is love axiomatised: the most principled way to spread and enact all the things that utilitarianism is unfavourably contrasted with.

I'm also curious to read some properly thoughtful and ingenious objections, beyond the familiar ones.

### History of Philanthropy

If you're a philanthropic movement and you want to avoid the mistakes and retrace the successes of your predecessors, a good start might be learning about the history of philanthropy. So I'm pro more books that tell the story of (e.g. 20th c.) philanthropy, and which try to draw out actionable lessons.

Open Philanthropy have made very good inroads on researching the [history of philanthropy](https://www.openphilanthropy.org/research/history-of-philanthropy). They report that this research "has contributed significantly to our [picture of what great giving looks like](https://www.openphilanthropy.org/research/insights-on-philanthropy)". In particular, they say that it (i) inspired ambition; (ii) suggested the value of creating rather than delegating new nonprofits; and (iii) suggested "the possibility of creating change by helping a nascent field grow even when there’s no apparent political opportunity". They also say that the most useful book they found was [*Casebook for The Foundation: A Great American Secret*](https://www.goodreads.com/book/show/10310142-casebook-for-the-foundation). I would add that *[Philanthropy: From Aristotle to Zuckerberg](https://www.goodreads.com/book/show/44452989-philanthropy)* also looks relevant.

Some people who know their stuff on this subject are [Benjamin Soskis](https://twitter.com/BenSoskis) and [Rhodri Davies](https://twitter.com/Rhodri_H_Davies). The website [HistPhil](https://histphil.org/about/) is is a web publication on the history of the philanthropic and nonprofit sectors

Stripe Press commissioning editor Tamara Winter [also recently tweeted about this question.](https://twitter.com/_TamaraWinter/status/1338978201842814976)

Beyond philanthropy, I'd also love to read a book about something like *social movements which achieved an outsized impact*. For example, the group of early [neoliberals](https://en.wikipedia.org/wiki/Neoliberalism) clustered around Chicago achieved a wild amount amount of influence, partly through the so-called '[Chicago boys](https://en.wikipedia.org/wiki/Chicago_Boys)'. How did that happen?

In a meta turn, I'd also be interested to read about the *books* that did the most to change the world, especially in a positive way. Less interested in "this famous person wrote a book, so that book must've been important", and more interested in "this book you likely haven't heard of appears to have actually influenced one or many important and consequential decisions".

### A Verbal History of EA

Effective altruism has reached a point where more people will be asking about its own history<Sidenote>Like journalists without much context, teetering between accuracy and slander.</Sidenote>. But everything happened too quickly for anyone to take notes. The records are there, but scattered across [forum](https://forum.effectivealtruism.org/) and blog posts. As EA grows, having a canonical source for the early history of EA will become more important.

But that's not the real reason I want to see a history of EA. The real reason is that it's an awesome story. It's funny, and surprising, and heartwarming, and inspiring.

One way this could get written is if a relative outsider (a journalist or writer) does some interviews, reads a few Wikipedia pages, and writes a story based on their view from the outside (like [Tom Chivers](https://www.goodreads.com/en/book/show/44154569) did for AI safety). This could be very good, but I am more compelled by the idea of a *verbal history* — a collection of voices woven together by an editor. [*Valley of Genius*](https://www.goodreads.com/book/show/36382335-valley-of-genius) does this for Silicon Valley and it's just epic. I hear [*Hackers*](https://www.goodreads.com/book/show/56829.Hackers) is very similar.

To be specific: imagine something like 50–150 interviews with people who were close to different parts of EA — who were in Oxford when [Giving What We Can](https://www.givingwhatwecan.org/) got started, who were around the Bay when things spread west and collided with rationalist people and [80,000 Hours](https://80000hours.org/) got started, and so on. The author narrates the broad strokes at the beginning of each chapter, and 90% of the remaining text is quotes from those interviews. Strange, amusing, nerdy minutiae are appreciated.

Would this book be *impactful*? I'm not convinced — seems more like a time-consuming exercise in naval gazery. But for the somewhat narrow audience that might appreciate such a book, I think it would be completely delightful.

### Exploring Utopia

A decent first approximation of certain [flavours](https://forum.effectivealtruism.org/posts/myp9Y9qJnpEEWhJF9/linch-s-shortform?commentId=E7383cdAjeKBub3Ks) of longtermism is *something* like 'maximise P(Utopia)'<Sidenote label='utopian'>For what it's worth I don't think this makes those versions of longtermism objectionably 'utopian', because they don't prescribe some particular utopia, they're just committed to a view along the lines of "the future *could* be radically better, let's protect that potential, and leave it to our descendants to build that future in all of its details."</Sidenote>. Longtermists and effective altruists are right to point out that we do not need a granular picture of 'utopia' to aim towards, so long as we can fix problems today and secure a future in which we eventually have the space to do that thinking. But writing about utopia could still make sense now, for a few reasons:

- Some of the most destructive social and political movements from history can be described as 'utopian', and it could pay to understand how those utopian visions translated into bloodshed and disarray.
  - On a smaller and less calamitous scale, history is littered with [failed experiments](https://en.wikipedia.org/wiki/List_of_American_utopian_communities) in communal/utopian living. I'd love to learn more about them, and learn about cross-cutting reasons they failed.
- As Holden Karnofsky [writes](https://www.cold-takes.com/why-describing-utopia-goes-badly/): "When thinking about the value of [ensuring that humanity continues to exist](https://theprecipice.com/) and/or [successfully navigating what could be the most important century](https://www.cold-takes.com/most-important-century/), it seems important to consider *how good things could be if they go well*, not just how bad things could be if they go poorly [...] In particular, I think it's liable to make us *fail to feel the full importance of what's at stake.*"
- Yet, it's [very difficult](https://www.cold-takes.com/why-describing-utopia-goes-badly/) to describe utopias in any kind of compelling or convincing detail. I'd like to see more writing about *why*, and also more efforts to *do better*.
- The history of attempts to describe utopia is going to roughly track a history of how we used to imagine the (actual) future, and knowing about this intellectual history could help us do that imagining better.
- I have spoken to smart people who just don't believe life can get much better than the standard of living for healthy people in relatively wealthy, stable parts of the world. A book about utopia might be aimed at persuading people that there is something more to hold out for.<Sidenote label='something-more'>For instance by looking into the literature on peak experiences.</Sidenote> Alternatively, maybe those people have a point. In which case, a book about utopia could try understanding *why* life can't feasibly get much better than this.

You could imagine a book which begins with the intellectual history, and mixes in stories about the practical experiments inspired by them, plus the events which shaped the intellectual history in turn. Then it could turn to actually sincerely trying to fill in some details about what an actual, achievable, conceivable utopia could look like. This means digging into some social science and speculative engineering.

## Megaprojects idea contest

Recently, it has become possible to consider EA projects which could scale to absorb hundreds of millions of dollars<Sidenote label='mega project'>So-called 'megaprojects'.</Sidenote>, because very large donors have stepped onto the scene and may be able to fund them. This effectively opens up a new category of project which previously was mostly not worth taking very seriously<Sidenote label='Aird'>Michael Aird has [written an excellent post](https://forum.effectivealtruism.org/posts/dZ8smFHpp4PGeFMF4/why-and-how-to-be-excited-about-megaprojects) nuancing exactly what this implies.</Sidenote>. Some ideas which could scale hugely are floating around, but perhaps there hasn't yet been enough of a systematic push to generate and (especially) sort them.

Like with ideas for a book, generating ideas for such massively scalable projects might be the easiest part — most of what matters lies in refining the ideas and then executing on them. But ideas are also cheap, so to pass over an idea for a highly impactful project of this kind would amount to an egregious missed opportunity.

One natural suggestion, then, is to announce a contest for 'megaproject' ideas. Entrances should include the case for impact, key uncertainties, and (crucially) possible harms. I'm not sure who would make a great judge, but perhaps folks with grant evaluation experience, or experienced forecasters. *Or* perhaps there is a way to have the community score the submissions. I also wonder about creative incentive models here, such as by paying submissions a bonus if the suggestion is both appreciably novel, and an appreciably similar version ends up being funded.

Are all the best 'megaproject' ideas fairly obvious? Or are some ideas, or variants on them, hiding from plain view? In other words: how expansive is the space of ideas? I'm not sure. And to the extent I'm not sure, it seems worth really checking for excellent but previously hidden ideas, especially since (again) ideas are cheap.

After soliciting ideas, I'd be interested in fleshing out, say, a dozen of the top proposals. Then you could imagine a round of evaluation, where perhaps teams of forecasters could independently score each proposal along various metrics, such as "by how many percentage points of percentage points could this reduce the chance of existential catastrophe this century?".

Since originally writing this idea down, the FTX Future Fund have of course [announced a very similar prize](https://forum.effectivealtruism.org/posts/KigFfo4TN7jZTcqNH/the-future-fund-s-project-ideas-competition). This is great news, of course. I'm unsure if it makes this idea redundant, or whether there could still be reason and interest to run a contest on the EA Forum.

## One-on-one advice matchmaking platform

If you have been following developments in the world of effective altruism recently, you will have noticed that money has suddenly become a whole lot cheaper relative to other things. Among other things, this means is that *skills* and *new people with the appropriate skills and context*<Sidenote label='context'>As in, thorough understanding of effective altruism and the field in which they work. Specifically the understanding that is hard to glean from the 'outside'. Context, after all, [is that which is scarce](https://marginalrevolution.com/marginalrevolution/2021/12/context-is-that-which-is-scarce.html)</Sidenote> have become far more valuable (in monetary terms); and at current margins we're bottlenecked by skills and people far more than money.

A rapid way to bring in new people, to vet them, and ultimately to begin trusting them to work autonomously, is to connect them with experienced EAs. I think even one-off calls can prove extremely influential in this way: a smart, motivated person can read a bunch about EA online, but the thing which can enable them to begin doing useful work is often speaking with other human beings doing that kind of work. However, the way in which more experienced EAs are matched up with new people is a little unsystematic and ad-hoc. It is possible to reach out to experienced EAs on initiative, but relying on initiative like this favours (i) confident and outgoing people over people who underrate their own ability or are otherwise shy about asking for advice; and (ii) people who are already well-connected (e.g. already went to a university with lots of EA alumni).

One exception is that the 80k careers advising team do a great job at connecting their advisees with experience people in their network. But this is limited: the 80k team can't be expected to keep track of every potential connection.

From the perspective of more experienced folks, it's nice to have ways to do good outside of your job, and give back to the community that gave you a leg up. The default way to do this has been to give ≈ 10% of your income to [Giving What We Can](https://www.givingwhatwecan.org/). But I just said that money has become relatively less valuable, and getting new (skilled, trustworthy) people involved has become relatively more valuable. So perhaps we could consider a new norm of giving on the order of 2–5% of your work time to speaking with junior people.

Now here's a concrete idea to enable all that: an online platform where advisees and advisors can sign up and input their interests and focuses. Advisors can tell the platform how much time they're prepared to spend mentoring others, plus what times they are free to speak, and the platform can work some magic and schedule calls automatically. Think of it kind of like scalable, distributed 80k advising.

One nice feature of this idea is that it wouldn't require much 'core EA' time to build — it sounds like a fairly standard job for a few developers. But it could streamline much of the ad-hoccery and inefficiency of arranging calls like this, and help leverage a small amount of experienced time to generate a lot of new EAs.

## Nuclear advocacy organisation

Nuclear (fission) power is one of the most powerful, space-efficient, and scalable sources of clean energy we have. And counter to its unfortunate reputation, it is also among the [cleanest and safest](https://ourworldindata.org/safest-sources-of-energy).<Sidenote label='fusion'>And then there's fusion — it looks like private money is beginning to flood into fusion research now, and we're seeing [results](https://www.bbc.com/news/science-environment-60312633) from public and private research alike. In fact, my guess is that a small lab will get to [ignition](https://en.wikipedia.org/wiki/Fusion_ignition) before [ITER](https://en.wikipedia.org/wiki/ITER), the decabillion-Euro international fusion project. Working fusion would be incredible, but it's easy to overlook the more immediately useful R&D frontier of [small modular fission reactors](https://en.wikipedia.org/wiki/Small_modular_reactor), which promise a way to get more nuclear power onto the grid in less time and at less expense — two major barriers to investment in nuclear.</Sidenote> Unfortunately, nuclear power is way underused compared to its promise, especially in rich Western countries. I think it's hard to overstate how bad this is — if we're serious about majorly transitioning to clean energy, it looks like nuclear is close to necessary.<Sidenote label='security'>Not to mention the national security angle, which is salient at the time of writing (i.e. that more domestic production and less relying on oil or gas imports puts you in a stronger bargaining position w.r.t autocratic regimes).</Sidenote> And there's a long way to go — some countries seem to be losing nuclear power plants on net, by [decommissioning](https://abcnews.go.com/International/wireStory/correction-germany-nuclear-shutdown-story-82051054) [some of them](https://en.wikipedia.org/wiki/Diablo_Canyon_Power_Plant). 

As far as I see it, this is largely a product of (i) unhelpful regulation around nuclear power deployment and R&D; and relatedly (ii) public opinion, which still seems attached to images of glowing green goo, and a certain HBO [miniseries](https://www.hbo.com/chernobyl).

With a really concerted effort of just getting credible information into the open, perhaps it is possible to positively change popular opinion. Someone who is killing it at advocating for nuclear power in a really creative way is the inimitable [Isabelle Boemeke](https://www.highsnobiety.com/p/isabelle-boemeke-interview/). Maybe that calls for a big, concerted effort at making nuclear's reputation less toxic, for instance by supporting science communicators to make a bunch of educational videos.

A more targeted approach could work better. You could imagine a political advocacy org take with coming up with really sensible policy change to make it easier and more attractive to develop more and cheaper nuclear power — the ultimate goal being the regulatory apparatus becoming friendlier to innovation in nuclear power. I think the [Breakthrough Initiative](https://www.breakthroughenergy.org/) are doing this a bit.

Why would this be good? Shifting away from dirty energy should obviously be a priority, for reasons I don't need to elaborate on. From a [longtermist](https://longtermism.com/) perspective, keeping fossil fuels in the ground could make it more likely that civilisations recover from a collapse to pre-industrial levels of technology. There is also a great power conflict angle which has been made too salient recently: if you can build domestic nuclear power plants, you're going to rely less on natural gas and other fossil fuels from oil and gas exporting states you might otherwise prefer not to associate with.

However, how tractable is change here? Not very, is my guess. In the US, for instance, what the [NRC](https://www.nrc.gov/about-nrc.html) says goes. And my impression is that the NRC is not going to be fantastically persuadable.

So where are the big nonprofit nuclear power advocacy groups? In spite of the paragraph above I am genuinely a bit confused about this. One answer is that the industry itself is an extremely powerful lobbying force, so it would be hard to make a dent in their efforts. But perhaps a nonprofit lobbying group, with no financial stake, could achieve things that the industry lobbyist cannot; especially since I largely have in mind public information campaigns, films, books, protests, etc. The fact that the industry itself has not succeeded at these ends doesn't seem like strong evidence that they're not worth trying. What would it take for fission power to become more of a bipartisan<Sidenote label='jobs'>Clean energy *and* jobs!</Sidenote> talking point?

## Quadratic funding pools for EAs

[Quadratic funding](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3243656) is an extremely neat method for allocating funding between different public goods, according to a collective decision-making procedure. In short, you have a pool of money and a group of people, each with their own personal pot of money. Each person can suggest and personally give to a proposed public good, and others can also give to that project according to how much they (would) value it. Then the quadratic finding rule decides how the larger pool gets spent, in the following way: for each project, take the sum of the square roots of each individual contribution, and square it.
$$
(\sum_{i=1}^{n}\sqrt{c_i})^2
$$
Where $c$ is each contribution.

Roughly speaking, this means the pool of money subsidises each project, in addition to the individual donations, in a way which gives you a certain kind of optimal allocation — namely, optimal for the group if everyone distributed their money to get the most bang-for-buck for themselves once the funding pool subsidises that project according to the rule. I am no good at explaining this in a short and less obscure way, so if you're curious you could read Vitalik Buterin's [excellent explainer on quadratic voting and quadratic funding](https://vitalik.ca/general/2019/12/07/quadratic.html). Here is an excerpt:

> In any situation where Alice contributes to a project and Bob also  contributes to that same project, Alice is making a contribution to  something that is valuable not only to herself, but also to Bob. When  deciding *how much to contribute*, Alice was only taking into  account the benefit to herself, not Bob, whom she most likely does not  even know. The quadratic funding mechanism adds a subsidy to compensate  for this effect, determining how much Alice "would have" contributed if  she also took into account the benefit her contribution brings to Bob. 

I'm suggesting that perhaps we could experiment with something similar as a mechanism for funding projects in EA. People opt in, perhaps to multiple pools with different themes. They can suggest projects — by describing the idea and listing it — and then vote on each project by spending some of their pot of money. Perhaps this could be the person's own money, or perhaps everyone could be allocated the same amount of 'credits' at the start. How to allocate 'credits' is tricky — on one hand, you want people to feel like they're invested in the process going well, and to feel like they have a responsibility to do a certain amount of due diligence on each project. On the other hand, you want everyone to feel empowered to participate even if they can't afford to pitch in as much of their own money: this is not primarily or even secondarily supposed to be a fundraising mechanism.

Setting up the infrastructure for this seems like the heftiest task here — maybe a few months of dev time at least, even for an MVP.

As a point in favour, this kind of QF mechanism gives people an influence over where funds are allocated that is greater than their individual donation, and this may make individual donations more attractive or meaningful. Also, it works as a really nice way to allocate funds democratically — it's probably close to an ideal rule for a funding pool to be said to fairly represent the views of its donors. 

What about considerations against? One issue is that the subsidising pool needs to be fairly large compared to the sum of individual donations, but that actually doesn't seem like a dealbreaker in a world where money is not especially constrained.

*More* importantly, using quadratic funding as a way to decide what to fund doesn't quite line up with the original motivation for quadratic funding, which is where the people who provide for the public goods are the same people who benefit from them. This is the case where you can show that the allocation ends up being optimal in a certain sense. But in the EA version, the beneficiaries barely overlap with the providers.<Sidenote label='beneficiaries'>Think animals, future generations, recipients of bed nets.</Sidenote> The game is less: people are all providing information about what they'd prefer, but rather: people are weighing in on what they think is best overall. And nobody claimed QF is the best way to share and aggregate that kind of information — it's normally going to be more (time and effort) efficient to trust a few decision-makers to act on behalf of many donors.

## Conclusion

As mentioned, email me if you're interested in helping join or start any of these projects! I'll aim to post a few more if people think that lists like these are useful. Footnotes (footnote, I guess) below.