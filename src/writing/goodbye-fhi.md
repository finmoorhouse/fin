---
title: Goodbye to FHI
permalink: /writing/fhi/
tags:
  - writing
date: 2024-04-20
shareImage: https://images.finmoorhouse.com/writing/goodbye-fhi/dandelion-5.webp
---

The lights are out at the Future of Humanity Institute. Anders Sandberg has put together a ['final report' on FHI](https://ora.ox.ac.uk/objects/uuid:8c1ab46a-061c-479d-b587-8909989e4f51), some oral history on how FHI grew up and what it was like to be there. There is also [a tombstone website](https://www.futureofhumanityinstitute.org/) with a short history and some greatest hits, publication-wise. {% note 'Do read those if you have the time' %}I'm largely stealing from them here anyway.{% endnote %}. But since I was fortunate enough to spend a couple years there, and to get to know many FHI people, I thought to add to the pile of commentary.

There are things to be said about what exactly led to FHI closing; but mostly not here. This'll just be a note of personal appreciation, and some sadness.

## Founding

>The most surprising thing is that the institute ever started.

If it started in one piece of writing, why not pin it to [Nick Bostrom](https://nickbostrom.com/)'s 1997 article ['Predictions from Philosophy?'](https://nickbostrom.com/old/predict). Progress on important (especially technological) dimensions is both *fast* and *exponential*: “scientific knowledge [is] doubling every 10 to 20 years since the second world war”, and “computer processor speed doubling every 18 months or so”. And this is not a familiar predicament to be in. If we want to look much further out into the future, we might need a grab-bag of predictive tools: some from science, mixed and combined as needed, and perhaps some of the more speculative and general-purpose tools from philosophy. So Bostrom asks whether there is room for "a philosophy whose aim is prediction", with a role for "the generalised scientist" to try making progress on questions that fall outside of narrower disciplines to otherwise be “consigned to [...] the popular press, or just ignored”.

There was also a fortuitous meeting, a year before. Here is a 1996 (!) {% note '[blog excerpt](https://www.hedweb.com/diar9605.htm)' %}[Archive link](https://web.archive.org/web/20010303214619/https://www.hedweb.com/diar9605.htm){% endnote %} from the philosopher David Pearce:

>I spend a most agreeable afternoon wandering around Kew Gardens in an old-fashioned organic VR. My companion and intellectual conscience is the polymathic Swedish-born [Niklas](https://web.archive.org/web/20010605061752/http://www.nickbostrom.com/). [...] Most academics' idea of constructive criticism stretches little further than the accusation they are too modest about their own abilities. Niklas, on the other hand, will both hear and deliver criticism in complete equanimity. [...] He should be meeting Anders in Stockholm next month. The cultural anthropologist in me eagerly awaits reports.

That is the same [Anders Sandberg](https://en.wikipedia.org/wiki/Anders_Sandberg) who would join FHI as it was established at the University of Oxford by Bostrom in 2005, originally intended as a 3-year project.

Here's my impression of the common thread of motivations that largely survived from founding onwards:

1. To study extremely zoomed-out questions of decision-relevance: what do we see for humanity's prospects when we look out from our vantage point in space and history? How might the world steer itself onto more hopeful trajectories?
2. To try to do this research *well*, meaning to aim for truth and importance and practical relevance. And turning that “what should we prioritise” lens onto the work itself, being prepared to {% note 'drop research directions and set out on new ones, as the territory comes into clearer view' %}See Hamming's [famous line of questioning](https://www.cs.virginia.edu/~robins/YouAndYourResearch.html): “What are the important problems of your field?”, “What important problems are you working on?”, and “If what you are doing is not important, and if you don't think it is going to lead to something important, why are you at Bell Labs working on it?”{% endnote %}. Academic prestige might proxy for this, but it shouldn't become the end goal.

This meant being prepared to do research which seem a bit fringe, or avant-garde, or frankly just weird — treading into territory normally associated with cranks and (for some reason) retired physicists. So the idea was less about advancing the far frontiers of already recognised research (as important as that is), but:

>to find things deserving of being recognized, show that they matter, invent the theoretical and conceptual tools needed to start to do useful work on them—and then (hopefully) hand it off to others as the topic matures.

That's from Anders' [final report](https://ora.ox.ac.uk/objects/uuid:8c1ab46a-061c-479d-b587-8909989e4f51) again.

## Some highlights

Things got started quickly in Oxford, largely thanks to the fiscal generosity of the late [Dr James Martin](<https://en.wikipedia.org/wiki/James_Martin_(author)>) (who gave his name to the [Oxford Martin School](https://en.wikipedia.org/wiki/Oxford_Martin_School)). Early research considered whole brain emulation, the [likelihood of global catastrophe](https://arxiv.org/pdf/astro-ph/0512204.pdf), the concept of a ['singleton'](https://nickbostrom.com/fut/singleton).

Academic discussion soon spilled over to the public square with the 2006 launch of a [collaborative blog called 'Overcoming Bias'](https://web.archive.org/web/20070105153523/https://www.overcomingbias.com/). Three years later, the scope of topics had inflated to require a new home again. Eliezer Yudkowsky, contributor to Overcoming Bias and longtime collaborator with FHI researchers, [moved to a new community blog](https://www.lesswrong.com/posts/S69ogAGXcc9EQjpcZ/a-brief-history-of-lesswrong-1) called ['LessWrong'](https://web.archive.org/web/20100412140505/http://lesswrong.com/).

In 2009, Nick Bostrom began work on a book on existential risks. One of the chapters kept growing until it was obvious its topic could consume a book of its own. This was the chapter on AI; the book became 2014's *[Superintelligence: Paths, Dangers, Strategies](https://www.goodreads.com/en/book/show/20527133)*. In the intervening years, papers were written and hires were made on AI safety. {% note 'FHI had stepped into a longstanding thread of thinking around intelligent machines' %}Indeed a thread Bostrom had contributed to already, as in [Bostrom (1997)](https://nickbostrom.com/superintelligence). [A note from his website](https://nickbostrom.com/): “I’ve now been alive long enough to have seen a significant shift in attitudes to these questions. Back in the 90s, they were generally regarded as discreditable futurism or science fiction - certainly within academia. They were left to a small set of “people on the Internet”, who were at that time starting to think through the implications of future advances in AI and other technologies, and what these might mean for human society.” For ‘people on the Internet’, see (for an influential if profligate example) the [Extropians mailing list](https://mtabarrok.com/extropia-archaeology). For much earlier examples, see [Turing (1950)](https://www.cs.ox.ac.uk/activities/ieg/e-library/sources/t_article.pdf) or [Good (1965)](http://incompleteideas.net/papers/Good65ultraintelligent.pdf) and [(1970)](https://www.tandfonline.com/doi/pdf/10.1080/00207237008709398). [This page](https://www.lesswrong.com/tag/history-of-ai-risk-thought) has many more examples.{% endnote %}, and became one of the most significant institutions for developing and popularising ideas around AI risk — ideas which have recently become far better known.

In 2017, FHI launched a 'Governance of AI Program' — led by Allan Dafoe — to study questions around policy and advanced AI, beyond the narrowly technical questions. As far as I understand, it was the first serious research effort on {% note "what's called ‘AI governance’," %}Specifically governance questions focused on making sure *transformative AI* goes well — the kind of AI that doesn't quite exist but might soon. There was and remains much work on policy questions around software, algorithms, and present-day AI.{% endnote %} now a burgeoning field. In 2021, that research group span out of FHI to become an independent think tank, the [Centre for the Governance of AI](https://www.governance.ai/).

From roughly 2018 onwards came a research group focused on biological risks. It made the case for biological risk reduction as a global priority, specifically because of engineered pandemics. Like with AI risk, this work was hardly the first to suggest that pandemics are bad, or that humanity might consider doing more to try preventing major pandemics. But it went further than most previous work in mounting a quantitative and systematic case for taking pandemic risk far more seriously. Shortly after, this team became very busy.

The very onset of Covid — early 2020 — also saw the release of Toby Ord's *[The Precipice](https://theprecipice.com/)*. The topic is 'existential risks' — risks that threatens the destruction of humanity’s longterm potential. There are chapters on unaligned AI and engineered pandemics, and more besides, but that organising theme explains what connects them as objects of special concern.

An even more recent development was two new hires and the formation of a research group working on ethical and strategic issues around digital minds. They helped organise a major multi-author report on ['Insights from the Science of Consciousness'](https://arxiv.org/abs/2308.08708).

{% image 'https://images.squarespace-cdn.com/content/v1/562652dbe4b05bbfdc596fd7/1615641917625-1TPVT29RZA49H0VGAXQJ/AS17-152-23420.jpg?format=1500w' 'Alt text' "One of Toby Ord's side projects was a digital restoration of some of the best photographs of Earth from the Apollo missions. This one is the last photograph of the whole Earth taken by human hands, from the 1972 Apollo 17 mission." "http://www.tobyord.com/earth"  %}

## Attitudes and impact

The writer Tim Urban [sometimes talks](https://waitbutwhy.com/?m=0&s=Idea+Lab) about 'Idea Labs', a kind of archetype for a research institution. From *[What's Our Problem](https://www.goodreads.com/en/book/show/102146148)*:

>People in an Idea Lab see one another as experimenters and their [ideas](https://twitter.com/waitbutwhy/status/1278035160454348800) as experiments [...] unearned conviction is a major no-no in an Idea Lab. So someone with a reputation for bias or arrogance or dishonesty will be met with a high degree of skepticism, no matter how much conviction they express.

From my impression of briefly being part of FHI (from 2020–2022), and from hearing about the earlier years, I think this gets at something FHI did well — this combination of (i) unusual openness and tolerance for 'weird' ideas, but (ii) unusual seriousness about getting them right.

[This thoughtful comment](https://www.lesswrong.com/posts/ydheLNeWzgbco2FTb/express-interest-in-an-fhi-of-the-west?commentId=osiadv9aSMfH7s6FW) mentions some more specific factors that made FHI distinctive. There was a lot of effort to cut through bullshit in group discussions — especially important when misguided ideas can survive and grow without the natural predator of {% note 'feedback from empirical tests' %}This is an especially thorny problem when you'd really prefer *not* to learn from experience. [Carl Sagan (1983)](https://www.jstor.org/stable/20041818): “Theories that involve the end of the world are not amenable to experimental verification—or at least, not more than once”{% endnote %}. And there was a vigilance about maintaining high research standards in hiring and visitors.

There have always been venues to discuss very big-picture topics like “what might end the world”, “are we in a simulation”, or “could we lose control of AI”. These include internet forums, smoke-filled dorm rooms, flyers handed out on the street, and (more recently) bro-coded podcast interviews. The scarce thing was to take these questions as seriously as any other technical or philosophical question, and conviction in the first place that it is possible to do much better than dorm-room discourse.

So earnestness and high standards were one thing. But that does not mean the many seminars and ad-hoc whiteboard discussions were not also, for a certain kind of technically and philosophically inclined nerd, unmissably fun. In terms of the breadth and interestingness of water cooler conversations, FHI was top-tier. Here's a small but representative sample of informal seminars I took notes from: on refining the concept of existential hope; on ethical considerations around [directed panspermia](https://en.wikipedia.org/wiki/Directed_panspermia); on whether the [Kelly criterion](https://en.wikipedia.org/wiki/Kelly_criterion) is a useful guide for humanity's ideal risk tolerance (no); on what's wrong with the [doomsday argument](https://en.wikipedia.org/wiki/Doomsday_argument); on which kinds of uncertainty count as good reasons for discounting the future; on the space of possible reforms to scientific institutions.

{% image 'https://lemanhattan.files.wordpress.com/2017/05/151023093956-01-futurists-restricted-super-43.jpg' 'Anders Sandberg stands by a whiteboard' "Anders stands by a whiteboard" "https://aleph.se/andart2/tag/future-of-humanity-institute/" %}

Also I think there are lessons about how to do {% note 'inderdisciplinary' %}Transdisciplinary? Multidisciplinary? Cross-disciplinary?{% endnote %} work {% note 'productively' %}Which isn't saying that FHI's outputs and perspectives were diverse on many important dimensions, they weren't. There was a house style to FHI's technical reports which was, well, *technical* and somewhat nerdy. And shorter thrift was given to nuanced social and political questions over questions where more confident claims or predictions looked feasible. Of course such a broad remit invites such “why don't you work on X” questions.{% endnote %}. Being 'interdisciplinary' is a selling point on grant applications, but sometimes has the feeling (to me) of a summit between delegates from two countries: lots of shaking hands and shows of mutual understanding and commonalities and gains from trade, but still the delegates go home to their own countries. I think FHI was less self-conscious here: the walls of the departments just mattered less simpliciter. Plus, there was a lot of openness to finding insights which had fallen somewhere in the cracks between zones of intense research pressure, {% note 'or far behind any specific 'frontier'' %}A nice example here is Toby Ord's paper on ['The Edges of Our Universe'](https://arxiv.org/ftp/arxiv/papers/2104/2104.01191.pdf) or his poster on the various [boundaries of a black hole](http://www.tobyord.com/writing/dissecting-a-black-hole). The concepts and maths in both cases are high-school level, if a notch above popular science nonfiction book level. So the incentives to write it are somehow rare both inside *and* out of the relevant academic fields, viz. cosmology or astronomy.{% endnote %}.

I really think it is worth appreciating the number and depth of insights that FHI can claim {% note 'significant credit for' %}Again, many of these ideas or close precursors were already in the water in *some* form, and in those cases the work was to majorly clarify or extend them. But isn't that true of most broadly significant philosophical ideas?{% endnote %}. In no particular order:

- The concept of [**existential risk**](https://existential-risk.com/), and arguments for treating **x-risk reduction as a global priority** (see: [_The Precipice_](https://theprecipice.com/))
- Arguments for **x-risk from AI**, and other philosophical considerations around superintelligent AI (see: [_Superintelligence_](https://www.goodreads.com/en/book/show/20527133))
- Arguments for the scope and importance of humanity's long-term future (since called **longtermism**)
- [**Information hazards**](https://nickbostrom.com/information-hazards.pdf)
- **Observer selection effects** and **‘**[**anthropic shadow’**](https://doi.org/10.1111/j.1539-6924.2010.01460.x)
- [**Bounding natural extinction rates**](https://pubmed.ncbi.nlm.nih.gov/31363134/) with statistical methods
- The [**vulnerable world hypothesis**](https://nickbostrom.com/papers/vulnerable.pdf)
- [**Moral trade**](http://amirrorclear.net/files/moral-trade.pdf)
- The [**moral imperative towards cost-effectiveness in global health**](http://amirrorclear.net/files/the-moral-imperative-towards-cost-effectiveness-in-global-health.pdf)
- [**Crucial considerations**](https://nickbostrom.com/lectures/crucial_final.pdf)
- The [**unilteralist's curse**](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4959137/)
- [**Dissolving**](https://arxiv.org/abs/1806.02404) the Fermi paradox
- The [**reversal test**](https://nickbostrom.com/ethics/statusquo.pdf) in applied ethics
- '[**Comprehensive AI services'**](https://www.fhi.ox.ac.uk/wp-content/uploads/Reframing_Superintelligence_FHI-TR-2019-1.1-1.pdf) as an alternative to unipolar outcomes
- The concept of [**existential hope**](https://www.fhi.ox.ac.uk/Existential-risk-and-existential-hope.pdf)

Note how much of the literal terminology was coined on (one imagines) a whiteboard in FHI. “Existential risk” isn't a neologism, but I understand it was Nick who first suggested it be used in a principled way to point to the “loss of potential” thing. “Existential hope”, “vulnerable world”, “unilateralist's curse”, “information hazard”, all (as far as I know) tracing back to an FHI publication.

It's also worth restating on the areas of study that FHI effectively incubated, and which are now full-blown fields of research. It was early on technical research around AI risk, very early on AI governance research, early on getting more strategic clarity around biosecurity. And if research on digital minds and their implications grows to become something resembling a 'field', then the small team and working groups on digital minds can make a claim to precedence, as well as [early](https://nickbostrom.com/papers/experience.pdf) and [more](https://nickbostrom.com/propositions.pdf) [recent](https://arxiv.org/abs/2308.08708) published work. Especially for its size, FHI was staggeringly influential.

## Headwinds and lessons

Like I said, I don't think this is the place for a play-by-play account of what caused FHI to close. The two main reasons are that (i) I'm not a spokesperson for FHI, so it would just be inappropriate; (ii) I don't know many details; and (iii) I don't expect particular details to change the impression you'd get from reading the '[final report](https://ora.ox.ac.uk/objects/uuid:8c1ab46a-061c-479d-b587-8909989e4f51)'.

Another reason is that shortly after someone dies, it just feels most appropriate to trade appreciative stories of that person's life as a whole, not to launch into the grizzly details of their end-of-life health complications, or to tally up all the stupid missteps that person ever made.

That said, my personal sense is that contributing mistakes were made, in the sense that things could have gone better (for everyone) if the tape were replayed.

In particular, the ops situation [was dysfunctional](https://forum.effectivealtruism.org/posts/uK27pds7J36asqJPt/future-of-humanity-institute-2005-2024-final-report?commentId=CAt9FHfF3FmLxKFiz), in a way that might have been fixed while there was still a chance. In any case it seems worth **strongly** emphasising the work of the ops people to keep FHI running as long as it did, especially in the later years where the ratchet mechanism of a hiring freeze meant ops staff could leave but not be replaced. That ops work was largely invisible and interfacing with the bureaucracy was, by all accounts, suffocating. [See this comment](https://forum.effectivealtruism.org/posts/uK27pds7J36asqJPt/future-of-humanity-institute-2005-2024-final-report?commentId=jHAZEg7iZvBmGJdAh), it is worth reading.

To be clear, I don't think this happened because FHI researchers themselves were unusually demanding. Rather, the rationale was to build a kind of sheltered garden around the researchers, free as far as possible from hostile outside forces — and it fell almost entirely on the ops staff to provide that shelter.

Still, Carrick points out an tension in how to relate to all this: on one hand, the operations staff bore an inhuman burden; this should be widely known and thanked. On the other, this was not an overall state of affairs to encourage, or glorify, or repeat. Even success would have been Pyrrich.

Anders [puts it like this](http://aleph.se/andart2/personal/thoughts-at-the-end-of-an-era/):

>I often described Oxford like a coral reef of calcified institutions built on top of each other, a hard structure that had emerged organically and haphazardly and hence had many little nooks and crannies where colorful fish could hide and thrive. FHI was one such fish but grew too big for its hole.

<div style='max-width:220px; margin:auto'>
{% image 'https://images.finmoorhouse.com/writing/precipice/precipice.jpg' 'The Precipice and the Land Beyond' "A woodcut by Hilary Paynter for The Precipice" "https://www.woodengravers.co.uk/artists/paynter-hilary/#&gid=1&pid=14" %}
</div>

As for lessons, who knows? Something about what progress can be made in a small place in a relatively short span of time by very committed misfits relatively unconcerned about academic prestige. Something hackneyed like that.

Should there be another FHI? I'm not sure. Compared to the world in 2005, the attitude and subjects of FHI's research are in safer hands. Governments [are forming](https://www.gov.uk/government/publications/ai-safety-institute-overview/introducing-the-ai-safety-institute) entire new [departments](https://www.commerce.gov/news/press-releases/2024/04/us-commerce-secretary-gina-raimondo-announces-expansion-us-ai-safety) around some of the catastrophic risks few were theorising about then. There are many, many new research and advocacy initiatives. And alumni are doing amazing things — heading major think tanks, leading safety research at AI labs, advising governments. If FHI were a candle then it's since lit many flames, and the matchbox might not be needed again.


{% image 'https://res.cloudinary.com/cea/image/upload/f_auto,q_auto/v1/mirroredImages/g7KJEbTPQqAkkkZsq/hd7xwambxgmv4mslwo9t' 'More whiteboards' 'More whiteboards' "https://aleph.se/andart2/tag/future-of-humanity-institute/" %}



